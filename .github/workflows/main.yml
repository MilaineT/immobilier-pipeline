name: Data Pipeline

on:
  push:                   # Déclenchement à chaque push
    branches:
      - main
  schedule:
    - cron: "0 9 * * *"   # Exécution automatique tous les jours à 9h UTC
  workflow_dispatch:       # Exécution manuelle possible

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      # Étape 1 : Récupérer le repo
      - name: Checkout repo
        uses: actions/checkout@v3

      # Étape 2 : Installer Python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.13'

      # Étape 3 : Installer les dépendances nécessaires
      - name: Install dependencies
        run: |
          pip install pandas scrapy

      # Étape 4 : Lancer le spider Scrapy
      - name: Run spider
        run: scrapy runspider src/spider.py -o data/cleaned_data_paris50_fr.csv -t csv

      # Étape 5 : Lancer le script de nettoyage
      - name: Run cleaner
        run: python src/cleaner.py

      # Étape 6 : Commit et push les fichiers de données mis à jour
      - name: Commit cleaned data
        run: |
          git config --global user.name "github-actions"
          git config --global user.email "actions@github.com"
          git add data/cleaned_data_paris50_fr.csv data/cleaned_data_final.csv
          git commit -m "Mise à jour automatique des données" || echo "No changes to commit"
          git pull --rebase origin main
          git push
